# SUPER Data Factory, Databricks e Databricks Delta Lake
Azure Data Factory, Databricks e Databricks Delta Lake

Este repositório contém os arquivos do curso de Azure Data Factory.

O curso aborda os principais conceitos e funcionalidades do ADF, incluindo:

# Preparação do ambiente
Criação de um banco de dados SQL Azure
Criação de tabelas no banco de dados SQL Azure
Provisionamento do Azure Data Factory
Copy Data
Importação de dados de um arquivo CSV para o banco de dados SQL Azure
Importação de dados de um serviço Web para o banco de dados SQL Azure
DataFlow
Origem de dados
Componentes DataFlow
Parâmetros na Pipeline/Dataflow
Orquestração e Dependências
Pipelines
Triggers
Integration Runtime, LinkedService e Datasets
Integration Runtime
LinkedService
Datasets
Monitorando Cargas e Alertas
Monitoramento de cargas
Alertas
Preparação do ambiente

O primeiro passo para começar a usar o Azure Data Factory é preparar o ambiente. Isso inclui criar um banco de dados SQL Azure e criar tabelas no banco de dados. Também é necessário provisionar o Azure Data Factory.

Copy Data

O Copy Data é uma atividade do Azure Data Factory que permite importar dados de um local para outro. No curso, são abordados dois tipos de atividades de Copy Data:

Importação de dados de um arquivo CSV para o banco de dados SQL Azure
Importação de dados de um serviço Web para o banco de dados SQL Azure
DataFlow

O DataFlow é uma ferramenta do Azure Data Factory que permite realizar transformações de dados. O curso aborda os seguintes componentes do DataFlow:

Origem de dados
Origem de arquivo
Origem de banco de dados
Componentes DataFlow
Seleção/ColunaDerivada
Executando Pipeline/Dataflow
Agregação
Rank/Classificação
União
Divisão Condicional
Junção
Orquestração e Dependências

As pipelines são a unidade básica de trabalho do Azure Data Factory. Elas permitem orquestrar as atividades do ADF. No curso, são abordados os seguintes conceitos de orquestração e dependências:

Pipelines
Triggers
Integration Runtime, LinkedService e Datasets

O Integration Runtime é um componente do Azure Data Factory que permite conectar o ADF a diferentes origens de dados. O LinkedService é um objeto que armazena as informações de conexão a uma origem de dados. O Dataset é um objeto que armazena as informações sobre os dados que serão manipulados pelo ADF.

Monitorando Cargas e Alertas

O Azure Data Factory fornece recursos para monitorar cargas e enviar alertas. No curso, são abordados os seguintes conceitos de monitoramento e alertas:

Monitoramento de cargas
Monitoramento de pipeline
Monitoramento de DataFlow
Alertas
Conclusão

Este curso fornece uma visão geral dos principais conceitos e funcionalidades do Azure Data Factory. Ao final do curso, o aluno será capaz de:

Preparar o ambiente para usar o Azure Data Factory
Importar dados de diferentes origens para o Azure Data Factory
Realizar transformações de dados usando o DataFlow
Orquestrar as atividades do ADF usando pipelines
Conectar o ADF a diferentes origens de dados
Monitorar cargas e enviar alertas

O curso foi ministrado pelo Juracy Araujo de Almeida Junior, que é um especialista em engenharia de dados, Professor das pós-graduações das universidades CATÓLICA DO SALVADOR, UFBA, UNIRUY e ESTÁCIO FIB em disciplinas das áreas de Business Intelligence e Banco de Dados.

Curso disponível em https://www.udemy.com/course/curso-super-data-factory-databricks-e-databricks-delta-lake/
